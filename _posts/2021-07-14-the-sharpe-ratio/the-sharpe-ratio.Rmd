---
title: "VAST Challenge 2021 - Mini-Challenge 3 "
description: |
  A short description of the post.
author:
  - name: Tang Yue
    url: {}
date: 07-14-2021
output:
  distill::distill_article:
    self_contained: false
---


```{r setup,include=FALSE}
options(htmltools.dir.version=FALSE)
knitr::opts_chunk$set(fig.retina = 3,
                      echo = TRUE,
                      eval = TRUE,
                      message = FALSE,
                      warning = FALSE)
```

# 1. Overview

In the roughly twenty years that Tethys-based GAStech has been operating a natural gas production site in the island country of Kronos, it has produced remarkable profits and developed strong relationships with the government of Kronos. However, GAStech has not been as successful in demonstrating environmental stewardship.

In January, 2014, the leaders of GAStech are celebrating their new-found fortune as a result of the initial public offering of their very successful company. In the midst of this celebration, several employees of GAStech go missing. An organization known as the Protectors of Kronos (POK) is suspected in the disappearance, but things may not be what they seem.

On January 23, 2014, multiple events unfolded in Abila. You’ve been asked to come in to perform a retrospective analysis based on limited information about what took place. Your goal is to identify risks and how they could have been mitigated more effectively.

in this assignment we would provide some insights to help law enforcement from Kronos and Tethys.


# 2. Data preparation and Exploration 

### 2.1 Data Source

There are three dataset provides in Mini-Challenge 3 :

(1) Microblog records that have been identified by automated filters as being potentially relevant to the ongoing incident

(2) Text transcripts of emergency dispatches by the Abila, Kronos local police and fire departments.

(3) maps of Abila and background documents.


The data of Microblog and text transcripts of emergency dispatches are found in three segments:

Segment 1  :"csv-1700-1830.csv" - covers the time period from 1700 to 1830 Abila time on January 23.

Segment 2: "csv-1831-2000.csv'- covers the time period from 1830 to 2000 Abila time on January 23.

Segment 3: "csv-2001-2131.csv" - covers the time period from 2000 to shortly after 2130 Abila time on January 23. 



### 2.2 Install and load R package

First, we run the below code to set the environment.


In this assignment, the tidyverse, ggforce, GGally, plotly R and parcoords packages will be used, which could be seen from below code chunk.


```{r}
packages = c('tidyverse','dplyr','readr','tm','wordcloud','SnowballC',
             'UpSetR','ggplot2','topicmodels','stringr','clock', 'tidytext','tokenizers','DT')
for (p in packages){
  if(!require(p, character.only = T)){
  install.packages(p)
  }
  library(p,character.only = T)
}
```


### 2.3 Import dataset and combine data

The data of Microblog records and emergency calls are stored in separate csv files. And three csv files share same columns but data generate from different date, which is shown below.

![](1.)


Firstly, we need to combine 3 files into one consolidated file, which is necessary for the following analysis. In this step, package 'tidyverse' would be used.

The following R code shows the process of data consolidation, then three dataset with different have been integrated into one file.

```{r}
library(tidyverse)
table1 <- read.csv("csv-1700-1830.csv")
table2 <- read.csv("csv-1831-2000.csv")
table3 <- read.csv("csv-2001-2131.csv")
data <- rbind(table1, table2,table3)
```


Next, we would separate 'ccdata' and 'mbdata', they represent microblog record and emergency call center data collected by  the Abila, Kronos local police and fire departments.


```{r}
ccdata <- subset(data, type == "ccdata")
mbdata <- subset(data, type == "mbdata")

```



### 2.4  Modifying Date formate  

Date data in table were shown as scientific notation, so we firstly disabling scientific notation in R for further date processing.

```{r}
library(lubridate)

options(scipen = 999)

```

Next, package 'lubridate' would be used to convert data type from 'yyyymmddhhmmss' to 'yyyy-mm-dd hh:mm:ss', and create a new column 'date' in data, and the code chunk could be seen below.


```{r}
data$date.yyyyMMddHHmmss.<- as.character(data$date.yyyyMMddHHmmss.)
data$date <-  date_time_parse(data$date.yyyyMMddHHmmss.,
                                      zone = "",
                                      format = "%Y%m%d %H%m%s")
```

### 2.5 Remove Special Chacaters

In this step, we would like to clean special characters, including " @, < , ˜ " and none English words. Besides, There are a lot of repeat records which forward from other publisher that begin with "RT". To avoid the affect the word frequency, we move "RT" in ahead.


```{r}
data$message <- str_replace_all(data$message,fixed("@"),"")
data$message <- str_replace_all(data$message,fixed("<"),"")
data$message <- str_replace_all(data$message,fixed("˜"),"")
data$message <- str_replace_all(data$message,"[\u4e00-\u9fa5]+", "")
data$message <- str_replace_all(data$message,fixed("RT"),"")

```


### 2.6 Classify Record

Based on content of data provided, we classified records into three segments, Junk, Spam and Meaningful records. 

Class| Definition
------|-------------
**Junk**| referred to advertisements or financial purpose reports. The below code chunk is used to identify the spam reports.|
**Meaningful**| refers to informative records which spread real news about ongoing or impending events happen in Aliba.|
**Spam**|represents the no meaningful and irrelevant or inappropriate messages post online.|

The str_detect() is used to detect patterns of Junk records. The below code chunk is used to identify the junk reports.|

```{r}
junk <- data %>%
  filter(str_detect(message,"#artists|#badcreadit|#cars
  |#followme|#nobanks|#nobank|#meds|#cancer|#bestfood
  |#workAtHome|#gettingFired|#pharmacyripoff|#iThing
  |homeworkers.kronos|#abilasFinest|#hungry|Easy make.money|
  #iThing|visit this|#eastAbila link|#abilajobs|clickhere|
  #welksFurniture| #swat |#bugs|visit this link|this site")) 

junk <- data %>%
  filter(str_detect(author,"junkman|carjunkers|eazymoney|
  junk99902|junkieduck113|junkman377|junkman995"))
           
```

Using the same way, we detect Spam records by below code chunk.

```{r}
spam <- data %>%
  filter(str_detect(message,"Grammar||#POKlove#Lucio|#getoverit
  |#baa-baa| #HomelessAwareness|#trylove|#pictures
  |#nobodycares|#abilafire|#wishfulthinking|Viktor-E
  |#hogwash|#RememberElian|#standoff|work from home|
  #blackvansrules|#schaber|#abilacityPark|#downwithkronos"))

spam <- data %>%
  filter(str_detect(author,"KronosQuoth|Clevvah4Evah|FriendsOfKronos|
  jaquesjoyce101|klingon4real|michelleR|SaveOurWildlands|
  AbilaAllFaith|footfingers|GreyCatCollectibles|luvwool"))
```


Apart from the Spam and Junk, other emails would be  classified into meaningful records, which could be received by the below code chunk.

```{r}
meaningful <- anti_join(data,junk,by="message") 
meaningful <- anti_join(meaningful,spam,by="message")
```


```{r}
junk$class <- "junk"
spam$class <- "spam"
meaningful$class <- "meaningful"

```


```{r}
data_classed <- rbind(spam, junk, meaningful)

```

Next, we further process "#" in the tag which leave in the before pre-processing code chunk.

```{r}
data_classed$message <- str_replace_all(data_classed$message,fixed("#"),"")
```


### 2.7 Data Tokenization 

In this code chunk below, unnest_tokens() of tidytext package is used to split the dataset into tokens, while stop_words() is used to remove stop-words.

Tokenization step

```{r}
data_classed_token <- data_classed %>%
  unnest_tokens(word, message) %>%
  filter(str_detect(word, "[a-z']$"),
         !word %in% stop_words$word)
```


# 2.8 Data Visualization 

Q1.Using visual analytics, characterize the different types of content in the dataset. What distinguishes meaningful event reports from typical chatter from junk or spam? 

Count the frequent word in all data_class

```{r message=FALSE, include=FALSE}
data_classed_token %>%
  count(word, sort = TRUE)
```


Instead of counting individual word, you can also count words within by newsgroup by using the code chunk below.

```{r}
token_by_class <- data_classed_token %>%
  count(class, word, sort = TRUE) %>%
  ungroup()
```


```{r}
wordcloud(data_classed_token$word,
          #data_classed_token$n,
          max.words = 200)
```


```{r}
wordcloud(token_by_class$word,
          token_by_class$n,
         max.words = 200)
```

The code chunk below uses bind_tf_idf() of tidytext to compute and bind the term frequency, inverse document frequency and ti-idf of a tidy text dataset to the dataset.


```{r}
tf_idf <- token_by_class %>%
  bind_tf_idf(word,class, n) %>%
  arrange(desc(tf_idf))

```


*Visualising tf-idf as interactive table*


Table below is an interactive table created by using datatable().

```{r}
DT::datatable(tf_idf, filter = 'top') %>% 
  formatRound(columns = c('tf', 'idf', 
                          'tf_idf'), 
              digits = 3) %>%
  formatStyle(0, 
              target = 'row', 
              lineHeight='25%')

```

*Visualising tf-idf as Wordcloud*

```{r}
tf_idf %>%
  #filter(str_detect(class, "^sci\\.")) %>%
  group_by(class) %>%
  slice_max(tf_idf, 
            n = 12) %>%
  ungroup() %>%
  mutate(word = reorder(word, 
                        tf_idf)) %>%
  ggplot(aes(tf_idf, 
             word, 
             fill = class)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ class, 
             ncol = 2,
             scales = "free") +
  labs(x = "tf-idf", 
       y = NULL)
```

*Q2*

```{r}
fire_count <- data_classed_token %>% 
  filter(str_detect(word,"fire|Dancing Dolphin Department")) 
```








