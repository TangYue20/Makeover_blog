[
  {
    "path": "posts/welcome/",
    "title": "Welcome to R for Data Science and Analysis",
    "description": "Welcome to our new blog, R for Data Science and Analysis. We hope you enjoy \nreading what we have to say!",
    "author": [
      {
        "name": "Tang Yue",
        "url": {}
      }
    ],
    "date": "2021-07-14",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-07-14T11:34:38+08:00",
    "input_file": "welcome.knit.md"
  },
  {
    "path": "posts/2021-07-14-the-sharpe-ratio/",
    "title": "VAST Challenge 2021 - Mini-Challenge 3 ",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Tang Yue",
        "url": {}
      }
    ],
    "date": "2021-07-14",
    "categories": [],
    "contents": "\r\n1. Overview\r\nIn the roughly twenty years that Tethys-based GAStech has been operating a natural gas production site in the island country of Kronos, it has produced remarkable profits and developed strong relationships with the government of Kronos. However, GAStech has not been as successful in demonstrating environmental stewardship.\r\nIn January, 2014, the leaders of GAStech are celebrating their new-found fortune as a result of the initial public offering of their very successful company. In the midst of this celebration, several employees of GAStech go missing. An organization known as the Protectors of Kronos (POK) is suspected in the disappearance, but things may not be what they seem.\r\nOn January 23, 2014, multiple events unfolded in Abila. You’ve been asked to come in to perform a retrospective analysis based on limited information about what took place. Your goal is to identify risks and how they could have been mitigated more effectively.\r\nin this assignment we would provide some insights to help law enforcement from Kronos and Tethys.\r\n2. Exploration and preparation of Data\r\nThere are two dataset provides:\r\nMicroblog records that have been identified by automated filters as being potentially relevant to the ongoing incident\r\nText transcripts of emergency dispatches by the Abila, Kronos local police and fire departments.\r\nmaps of Abila and background documents.\r\nTwo dataset are stored in 3 csv files, they are csv-1700-1830.csv“,”csv-1831-2000.csv’ and “csv-2001-2131.csv”:\r\nThe data for Mini-Challenge 3 is found in three segments.\r\nSegment 1 covers the time period from 1700 to 1830 Abila time on January 23.\r\nSegment 2 covers the time period from 1830 to 2000 Abila time on January 23.\r\nSegment 3 covers the time period from 2000 to shortly after 2130 Abila time on January 23.\r\n2.1 Install and load R package\r\nFirst, we run the below code to set the environment.\r\nIn this assignment, the tidyverse, ggforce, GGally, plotly R and parcoords packages will be used, which could be seen from below code chunk.\r\n\r\n\r\npackages = c('tidyverse','dplyr','readr','tm','wordcloud','SnowballC',\r\n             'UpSetR','ggplot2','topicmodels','stringr','clock', 'tidytext','tokenizers','DT')\r\nfor (p in packages){\r\n  if(!require(p, character.only = T)){\r\n  install.packages(p)\r\n  }\r\n  library(p,character.only = T)\r\n}\r\n\r\n\r\n\r\n2.2 Import dataset and combine data\r\nThe data of Microblog records and emergency calls are stored in separate csv files. And three csv files share same columns but data generate from different date, which is shown below.\r\n1\r\nFirstly, we need to combine 3 files into one consolidated file, which is necessary for the following analysis. In this step, package ‘tidyverse’ would be used.\r\nThe following R code shows the process of data consolidation, then three dataset with different have been integrated into one file.\r\n\r\n\r\nlibrary(tidyverse)\r\ntable1 <- read.csv(\"csv-1700-1830.csv\")\r\ntable2 <- read.csv(\"csv-1831-2000.csv\")\r\ntable3 <- read.csv(\"csv-2001-2131.csv\")\r\ndata <- rbind(table1, table2,table3)\r\n\r\n\r\n\r\nNext, we would separate ‘ccdata’ and ‘mbdata’, they represent microblog record and emergency call center data collected by the Abila, Kronos local police and fire departments.\r\n\r\n\r\nccdata <- subset(data, type == \"ccdata\")\r\nmbdata <- subset(data, type == \"mbdata\")\r\n\r\n\r\n\r\n2.3 Data preprocessing\r\n2.3.1 Modifying Date formate\r\nDate data in table were shown as scientific notation, so we firstly disabling scientific notation in R for further date processing.\r\n\r\n\r\nlibrary(lubridate)\r\n\r\noptions(scipen = 999)\r\n\r\n\r\n\r\nNext, package ‘lubridate’ would be used to convert data type from ‘yyyymmddhhmmss’ to ‘yyyy-mm-dd hh:mm:ss’, and create a new column ‘date’ in data, and the code chunk could be seen below.\r\n\r\n\r\ndata$date.yyyyMMddHHmmss.<- as.character(data$date.yyyyMMddHHmmss.)\r\ndata$date <-  date_time_parse(data$date.yyyyMMddHHmmss.,\r\n                                      zone = \"\",\r\n                                      format = \"%Y%m%d %H%M%s\")\r\n\r\n\r\n\r\n\r\n\r\nglimpse(data$date)\r\n\r\n\r\n POSIXct[1:4063], format: NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ...\r\n\r\n2.3.2 Text proprecessing\r\nIn this step, we would like to clean text, including punctuation removal, converting lower case, stop word removal, extra white space removal and stemming.\r\nCleaning the text data starts with making transformations like removing special characters from the text. This is done using the tm_map() function to replace special characters like / and | with a space. The next step is to remove the unnecessary whitespace and convert the text to lower case.\r\n\r\n\r\ndata$message <- str_replace_all(data$message,'[[:punct:]]+', \"\")\r\n\r\ndata$message <- str_replace_all(data$message,fixed(\"@\"),\"\")\r\n\r\n#data$message <- str_replace_all(data$message,fixed(\"#\"),\"\")\r\n\r\ndata$message <- str_replace_all(data$message,fixed(\"<\"),\"\")\r\ndata$message <- str_replace_all(data$message,fixed(\"˜\"),\"\")\r\n\r\ndata$message <- str_replace_all(data$message,\"[\\u4e00-\\u9fa5]+\", \"\")\r\n\r\n\r\n\r\n2.3.3 Classify Record\r\nJunk referred to advertisements or financial purpose reports. The below code chunk is used to identify the spam reports.\r\n\r\n\r\njunk <- data %>%\r\n  filter(str_detect(message,\"#artists|#badcreadit|#cars|#followme|#nobanks|# nobank|#meds|#cancer|#bestfood|#workAtHome|#gettingFired|#pharmacyripoff|#iThing\r\n                    |homeworkers.kronos|#abilasFinest|#hungry|Easy make.money|\r\n                    #iThing|visit this|#eastAbila link|#abilajobs|clickhere|\r\n                    #welksFurniture| #swat |#bugs|visit this link|this site\")) \r\n\r\njunk <- data %>%\r\n  filter(str_detect(author,\"junkman|carjunkers|eazymoney|junk99902|junkieduck113\r\n  |junkman377|junkman995\"))\r\n\r\n\r\n\r\nSpam represents the no meaningful and irrelevant or inappropriate messages post online. The below code chunk is used to identify the spam reports.\r\n\r\n\r\nspam <- data %>%\r\n  filter(str_detect(message, \"Grammar||#POKlove#Lucio|#getoverit|#baa-baa| #HomelessAwareness|#trylove|#pictures|#nobodycares|#abilafire|#wishfulthinking|\r\n                    Viktor-E|#hogwash|#RememberElian|#standoff|work from home|#blackvansrules|#schaber|#abilacityPark|#downwithkronos\"))\r\n\r\nspam <- data %>%\r\n  filter(str_detect(author,\"KronosQuoth|Clevvah4Evah|FriendsOfKronos|\r\n                    jaquesjoyce101|klingon4real|michelleR|SaveOurWildlands|\r\n                    AbilaAllFaith|footfingers|GreyCatCollectibles|luvwool\"))\r\n\r\n\r\n\r\nApart from the Spam and Junk, other emails would be classified into meaningful records, which could be received by the below code chunk.\r\n\r\n\r\nmeaningful1 <- anti_join(data,junk,by=\"message\") \r\nmeaningful <- anti_join(meaningful1,spam,by=\"message\")\r\n\r\n\r\n\r\nNext, we further process “#” in the tag which leave in the before pre-processing code chunk.\r\n\r\n\r\njunk$message <- str_replace_all(junk$message,fixed(\"#\"),\"\")\r\nspam$message <- str_replace_all(spam$message,fixed(\"#\"),\"\")\r\nmeaningful$message <- str_replace_all(meaningful$message,fixed(\"#\"),\"\")\r\n\r\n\r\n\r\n2.3.4 Data Tokenization\r\nIn this code chunk below, unnest_tokens() of tidytext package is used to split the dataset into tokens, while stop_words() is used to remove stop-words.\r\n\r\n\r\njunk_token <- junk %>%\r\n  unnest_tokens(word, message) %>%\r\n  filter(str_detect(word, \"[a-z']$\"),\r\n         !word %in% stop_words$word)\r\n\r\n\r\n\r\nThe below code chunk was used for spam email tokenization.\r\n\r\n\r\nsapm_token <- spam %>%\r\n  unnest_tokens(word, message) %>%\r\n  filter(str_detect(word, \"[a-z']$\"),\r\n         !word %in% stop_words$word)\r\n\r\n\r\n\r\nIn same method, the below code chunk was used for meaningful email tokenization.\r\n\r\n\r\nmeaningful_token <- meaningful %>%\r\n  unnest_tokens(word, message) %>%\r\n  filter(str_detect(word, \"[a-z']$\"),\r\n         !word %in% stop_words$word)\r\n\r\n\r\n\r\n\r\n\r\njunk_token %>%\r\n  count(word, sort = TRUE)\r\n\r\n\r\n                          word  n\r\n1                        abila 35\r\n2                    followers  9\r\n3                       kronos  7\r\n4                       online  7\r\n5                        cheap  6\r\n6                       credit  6\r\n7                       refill  6\r\n8                         100s  4\r\n9                    badcredit  4\r\n10                       boost  4\r\n11                       check  4\r\n12                        easy  4\r\n13    fixitkronoscreditratings  4\r\n14    followerskronosgetpeople  4\r\n15                        free  4\r\n16                        hows  4\r\n17                          im  4\r\n18                      ithing  4\r\n19                        lost  4\r\n20                        meds  4\r\n21                        nuts  4\r\n22                prescription  4\r\n23                     program  4\r\n24                      rating  4\r\n25                       swear  4\r\n26                     article  3\r\n27      badprofileskronostacky  3\r\n28                    bestfood  3\r\n29                       bunch  3\r\n30                         car  3\r\n31                        cars  3\r\n32          carskronoschoppers  3\r\n33                       click  3\r\n34         dietkronosstarvenow  3\r\n35       drugskronoscheappills  3\r\n36                  medication  3\r\n37         medskronoscheapmeds  3\r\n38                        news  3\r\n39              pharmacyripoff  3\r\n40                    pictures  3\r\n41                   questions  3\r\n42                       sales  3\r\n43                       stuff  3\r\n44                        time  3\r\n45                        vans  3\r\n46                      weight  3\r\n47                          5k  2\r\n48                   abilajobs  2\r\n49                       cards  2\r\n50                     contest  2\r\n51     contestkronosfreeithing  2\r\n52        dateskronosclickhere  2\r\n53  easycreditkronosmorecredit  2\r\n54                      famous  2\r\n55                        haha  2\r\n56                       heard  2\r\n57                       heres  2\r\n58                        home  2\r\n59     howworkathomekronosscam  2\r\n60         makemoneykronoslink  2\r\n61                    millions  2\r\n62                          mo  2\r\n63                     nobanks  2\r\n64                        paid  2\r\n65                      people  2\r\n66                       rally  2\r\n67                   recommend  2\r\n68                        site  2\r\n69                    software  2\r\n70                       worth  2\r\n71                       youre  2\r\n72                      advise  1\r\n73                      artist  1\r\n74                     artists  1\r\n75                       blogs  1\r\n76                     booking  1\r\n77        cheapdrugskronosmeds  1\r\n78     chirpwatchkronoswatchme  1\r\n79                        city  1\r\n80                    discount  1\r\n81                       drugs  1\r\n82        fastjobskronostricks  1\r\n83                    feelings  1\r\n84       followmekronosartists  1\r\n85                       found  1\r\n86                         fyi  1\r\n87                gettingfired  1\r\n88                     grammar  1\r\n89                   hesitates  1\r\n90        hottipskronostrading  1\r\n91                       human  1\r\n92                  kronosstar  1\r\n93                        link  1\r\n94                       march  1\r\n95                       month  1\r\n96                    musician  1\r\n97                       nasty  1\r\n98                        park  1\r\n99                       photo  1\r\n100             picskronospics  1\r\n101                        pok  1\r\n102                   pokrally  1\r\n103                    profile  1\r\n104                   required  1\r\n105                 savepeople  1\r\n106                  spreading  1\r\n107                       tear  1\r\n108                trafficking  1\r\n109                     viewed  1\r\n110                      visit  1\r\n111                   watching  1\r\n112                      water  1\r\n113                    website  1\r\n114                       whos  1\r\n115                 workathome  1\r\n\r\n\r\n\r\nwords_junk_author <- junk_token %>%\r\n  count(author, word, sort = TRUE) %>%\r\n  ungroup()\r\n\r\n\r\n\r\n\r\n\r\nwordcloud(words_junk_author$word,\r\n          #words_by_author$n,\r\n          max.words = 300)\r\n\r\n\r\n\r\n\r\n\r\n\r\nwordcloud(junk_token$word,\r\n          #junk_token$n,\r\n          max.words = 300)\r\n\r\n\r\n\r\n\r\nThe code chunk below uses bind_tf_idf() of tidytext to compute and bind the term frequency, inverse document frequency and ti-idf of a tidy text dataset to the dataset.\r\n\r\n\r\n#tf_idf <- words_by_author %>%\r\n#  bind_tf_idf(word, n) %>%\r\n#  arrange(desc(tf_idf))\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-07-14-the-sharpe-ratio/the-sharpe-ratio_files/figure-html5/unnamed-chunk-17-1.png",
    "last_modified": "2021-07-16T21:57:47+08:00",
    "input_file": "the-sharpe-ratio.knit.md"
  }
]
