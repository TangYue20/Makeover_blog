[
  {
    "path": "posts/welcome/",
    "title": "Welcome to R for Data Science and Analysis",
    "description": "Welcome to our new blog, R for Data Science and Analysis. We hope you enjoy \nreading what we have to say!",
    "author": [
      {
        "name": "Tang Yue",
        "url": {}
      }
    ],
    "date": "2021-07-14",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-07-14T11:34:38+08:00",
    "input_file": "welcome.knit.md"
  },
  {
    "path": "posts/2021-07-14-the-sharpe-ratio/",
    "title": "VAST Challenge 2021 - Mini-Challenge 3 ",
    "description": "Using Visual Analytics to Support Decision Making to Solve The kronos Incident.",
    "author": [
      {
        "name": "Tang Yue",
        "url": {}
      }
    ],
    "date": "2021-07-14",
    "categories": [],
    "contents": "\r\n1. Overview\r\nThe 2021 Visual Analytics Science and Technology (VAST) Challenge presented researchers with a single fictitious scenario: the disappearance of staff members of the GASTech oil and gas company on location on the island of Kronos. A group named the Protectors of Kronos (POK) was the prime suspect in the disappearance. Three mini-challenges and a grand challenge were offered. For more information, please see VAST Challenge 2021.\r\nThis module will research Mini-Challenge 3 which includes multiple types of text data for participants to feature real-time streaming social media and emergency service data for participants to provide hostage and kidnapper information.This challenge has 3 tasks and questions and asked the participants to integrate results to evaluate the changing levels of risk to the public and recommend actions.\r\n2. Data preparation and Exploration\r\n2.1 Data Source\r\nThere are three dataset provides in Mini-Challenge 3 :\r\nMicroblog records that have been identified by automated filters as being potentially relevant to the ongoing incident\r\nText transcripts of emergency dispatches by the Abila, Kronos local police and fire departments.\r\nmaps of Abila and background documents.\r\n\r\nThe data of Microblog and text transcripts of emergency dispatches are found in three segments:\r\nSegment 1 :“csv-1700-1830.csv” - covers the time period from 1700 to 1830 Abila time on January 23.\r\nSegment 2: \"csv-1831-2000.csv’- covers the time period from 1830 to 2000 Abila time on January 23.\r\nSegment 3: “csv-2001-2131.csv” - covers the time period from 2000 to shortly after 2130 Abila time on January 23.\r\n\r\n2.2 Install and load R package\r\nIn this module, the tidyverse, ggforce, GGally, plotly R and parcoords packages will be used, which could be seen from below code chunk.\r\n\r\n\r\npackages = c('tidyverse','dplyr','readr','tm','lubridate','wordcloud','SnowballC', 'UpSetR','ggplot2','ggwordcloud','topicmodels','stringr','clock', 'tidytext','tokenizers','DT')\r\nfor (p in packages){\r\n  if(!require(p, character.only = T)){\r\n  install.packages(p)\r\n  }\r\n  library(p,character.only = T)\r\n}\r\n\r\n\r\n\r\n2.3 Import dataset and combine data\r\nThe data of Microblog records and emergency calls are stored in separate csv files. And three csv files share same columns but data generate from different date, which is shown below.\r\n\r\nFirstly, we need to combine 3 files into one consolidated file, which is necessary for the following analysis. In this step, package tidyverse would be used.The following R code shows the process of data consolidation, then three datasets with different date have been integrated into one file.\r\n\r\n\r\nlibrary(tidyverse)\r\ntable1 <- read.csv(\"csv-1700-1830.csv\")\r\ntable2 <- read.csv(\"csv-1831-2000.csv\")\r\ntable3 <- read.csv(\"csv-2001-2131.csv\")\r\ndata <- rbind(table1, table2,table3)\r\n\r\n\r\n\r\n2.4 Modifying Date format\r\nconverting character objects to dates can be made easier by using the lubridate package, which would be used to convert date type from ‘yyyymmddhhmmss’ to ‘yyyy-mm-dd hh:mm:ss’, and create a new column ‘date’ in data, and the code chunk could be seen below.\r\n\r\n\r\ndata$date.yyyyMMddHHmmss. <- ymd_hms(data$date.yyyyMMddHHmmss.)\r\n\r\n\r\n\r\nRename “date.yyyyMMddHHmmss.” in data into “date” with concise format.\r\n\r\n\r\nnames(data)[names(data) == \"date.yyyyMMddHHmmss.\"] <- \"date\"\r\n\r\n\r\n\r\n2.5 Remove Special Chacaters\r\nIn this step, we would like to use package stringr to clean special characters, including \" @, < , ˜ \" and none English words.\r\nBesides, there are a lot of repeat records which forward from other publisher that begin with “RT”. The repeat records represent one review or voice, so we keep it. However, avoiding meaningless “RT” affect the word frequency, we move “RT” in ahead.\r\n\r\n\r\ndata$message <- str_replace_all(data$message,fixed(\"@\"),\"\")\r\ndata$message <- str_replace_all(data$message,fixed(\"<\"),\"\")\r\ndata$message <- str_replace_all(data$message,fixed(\"˜\"),\"\")\r\ndata$message <- str_replace_all(data$message,\"[\\u4e00-\\u9fa5]+\", \"\")\r\ndata$message <- str_replace_all(data$message,fixed(\"RT\"),\"\")\r\n\r\n\r\n\r\nFor convenience of further analysis, we separate ‘ccdata’ and ‘mbdata’ from “data”, they respectively represent microblog record and emergency call center data collected by the Abila, Kronos local police and fire departments.\r\n\r\n\r\nccdata <- subset(data, type == \"ccdata\")\r\nmbdata <- subset(data, type == \"mbdata\")\r\n\r\n\r\n\r\n2.6 Classify Record\r\nBased on content of data provided, we classified records into three segments, Junk, Typical_Chatter and Meaningful records, using stringr, dplyrpackage. Because emergency call center data is credible and reliable, here we just classify Microblog data into three classes.\r\nClass\r\nDefinition\r\nJunk\r\nreferred to advertisements or financial purpose reports.\r\nMeaningful\r\nrefers to informative records which spread real news about ongoing or impending events happen in Aliba.\r\nTypical Chatter\r\nrepresents no meaningful and irrelevant or inappropriate messages post online.\r\nclassify Junk records: The str_detect() function is used to detect patterns of Junk records. And the below code chunk is used to identify the junk reports.|\r\n\r\n\r\njunk <- mbdata %>% filter_at(.vars = vars(message, author), \r\nany_vars(str_detect(., pattern = \"#artists|#badcreadit|#cars\r\n  |#followme|#nobank|#meds|#cancer|#bestfood|#Grammar|cars.kronos\r\n  |#workAtHome|#gettingFired|#pharmacyripoff|#iThing|sellstuff\r\n  |homeworkers.kronos|#abilasFinest|#hungry|Easy make.money|Officia1AbilaPost|\r\n  visit this|#eastAbila link|#abilajobs|clickhere|#abilajobs|\r\n  #welksFurniture|#swat|#bugs|visit this link|this site|\r\n  junkman|carjunkers|eazymoney|junk99902|junkieduck113\r\n  |junkman377|junkman995|KronosQuoth|Clevvah4Evah\")))\r\n\r\n\r\n\r\n\r\n\r\nmbdata <- anti_join(mbdata,junk,by=\"message\") \r\n\r\n\r\n\r\nClassify Spam records: Using the same way, we detect Spam records by below code chunk and classify corresponding emails into Spam class.\r\n\r\n\r\nmeaningful<- mbdata %>% filter_at(.vars = vars(message, author), \r\nany_vars(str_detect(., pattern =\"#gowaway|#POKrally|van|gun|fire\r\n|rally|shooting|#AlibaPost|#CentralBulletin|\r\n#POKRallyinthePark|#IntNews|#nopeace|#IntNews|#KronosStar|#stuck|\r\n#Gastech employees|#POK connection|#Abila|#abilafire|#POK| #NewsOnline|#hurtfirefighte|#rally|#pokrally|#SavePeople\r\n|#terrorists|#tellme| #praying|@carlyscoffee|@dancingdolphin|\r\nterorrist|cop|#abilawatcher|#IntNews|#KronosStar|#gettingFired\r\n|#GelatoGalore|#helpu|#HI|#newsOnline|#DancingDolphinFire|\r\n#jerkdrivers|#stopby|#standoff|#abilaFire|#hurtfirefighter\r\n|#norest|#firstresponders|#IntNews|#AFDHeroes|#APD|#shooting\r\n|#kidnapping|#troubleatgelato|#Abila|#burnabila|#hostage|\r\n#repent|#dansingdolfinfire|#jerkdrivers|#AFD|#APDSWAT|\r\n#shooter|#HELPME|#fire|#thanksAFD\")))\r\n\r\n\r\n\r\nClassify Meaningful records: Apart from the Spam and Junk, the rest of emails would be classified into meaningful records, which could be received by the below code chunk.In this step, anti_join() function in dplyr package would be used.\r\n\r\n\r\nTypical_Chatter <- anti_join(mbdata,junk,by=\"message\") \r\nTypical_Chatter <- anti_join(Typical_Chatter,meaningful,by=\"message\")\r\n\r\n\r\n\r\nNow, all records in Microblog data have been labeled with responding class- Junk, Meanigful and Spam.\r\nNext, we add “class” as a new column to represent the label of every record.\r\n\r\n\r\njunk$class <- \"junk\"\r\nTypical_Chatter$class <- \"Typical_Chatter\"\r\nmeaningful$class <- \"meaningful\"\r\n\r\n\r\n\r\nUsing below code chunk, we combine Junk, Spam and Meaningful records into one dataframe which was renamed as data_classed. In this preprocessing step, the rbind function of dplry package would be used.\r\n\r\n\r\nmbdata_classed <- rbind(junk, Typical_Chatter, meaningful)\r\n\r\n\r\n\r\nIn the previous pre-processing step, we keep # as a tag to identify Junk, Spam and meaningful records. After records classifying, we move “#” in the tag using below code chunk.\r\n\r\n\r\nmbdata_classed$message <- str_replace_all(mbdata_classed$message,fixed(\"#\"),\"\")\r\n\r\n\r\n\r\n2.7 Data Tokenization and Word Counts\r\nAfter cleaning the text data, the next step is to segment sentence and count the occurrence of each word, to identify frequent words in records. The below code chunk shows how Multiple functions used to achieve this results:\r\nunnest_tokens() function from tidytext package is used to split the sentences into tokens,\r\nstop_words() is used to remove stop-words.\r\nstr_detect() is used to detect the presence or absence of a pattern in string.\r\nfilter() of dplyr package is used to subset a data frame, retaining all rows that satisfy the specified conditions.\r\n\r\n\r\nmbdata_classed_token <- mbdata_classed %>%\r\n  unnest_tokens(word, message) %>%\r\n  filter(str_detect(word, \"[a-z']$\"),\r\n         !word %in% stop_words$word,\r\n         !word %in% c(\"aliba\",\"abila\"))\r\n\r\n\r\n\r\nThe most common words in entire dataset could be seen from below code chunk.\r\n\r\n\r\nmbdata_classed_token %>%\r\n  count(word,sort = TRUE) %>%\r\n  top_n(15) \r\n\r\n\r\n                    word    n\r\n1               pokrally 1373\r\n2             kronosstar  931\r\n3                    pok  494\r\n4              abilapost  412\r\n5                   fire  397\r\n6                  rally  300\r\n7                 police  294\r\n8                   life  210\r\n9                 people  199\r\n10       centralbulletin  186\r\n11 homelandilluminations  183\r\n12                   apd  164\r\n13               grammar  157\r\n14               dancing  143\r\n15               success  141\r\n\r\n\r\n\r\nmbdata_classed_token %>%\r\n  count(word, sort = TRUE) %>%\r\n  filter(n > 140) %>%\r\n  mutate(word = reorder(word, n)) %>%\r\n  ggplot(aes(n, word, fill = word)) +\r\n  theme(legend.position = \"none\")+\r\n  geom_col() +\r\n  labs(y = NULL)\r\n\r\n\r\n\r\n\r\nGenerate the word cloud The importance of words can be illustrated as a word cloud as follow :\r\n\r\n\r\nset.seed(1234)\r\nwordcloud(words = mbdata_classed_token$word,\r\n          #freq = mbdata_classed_token$n,\r\n          min.freq = 10,\r\n          max.words = 200, \r\n          random.order=FALSE, rot.per=0.35,\r\n          colors=brewer.pal(8, \"Dark2\"))\r\n\r\n\r\n\r\n\r\n3. Data Visualization\r\nQuestion 1:\r\nUsing visual analytics, characterize the different types of content in the dataset. What distinguishes meaningful event reports from typical chatter from junk or spam?\r\nFirstly, we count words for three classes- Junk, Spam, Meaningful using the code chunk below, then the frequent words in tree record class could be seen.\r\n\r\n\r\ntoken_by_class <- mbdata_classed_token %>%\r\n  count(class, word, sort = TRUE) %>%\r\n  group_by(class) %>%\r\n  top_n(15) %>%\r\n  ungroup()\r\n\r\n\r\n\r\n\r\n\r\nset.seed(1234)\r\ntoken_by_class %>%\r\n  filter(n > 0) %>%\r\nggplot(aes(label = word,\r\n           size = n)) +\r\n  geom_text_wordcloud() +\r\n  theme_minimal() +\r\n  facet_wrap(~class)\r\n\r\n\r\n\r\n\r\nThe code chunk below uses bind_tf_idf() of tidytext to compute and bind the term frequency, inverse document frequency and ti-idf of a tidy text dataset to the dataset.\r\n\r\n\r\ntf_idf <- token_by_class %>%\r\n  bind_tf_idf(word,class, n) %>%\r\n  arrange(desc(tf_idf))\r\n\r\n\r\n\r\nVisualising tf-idf as interactive table\r\nTable below is an interactive table created by using datatable(). The DT table can be used to review the frequency, tf-idf of common words of three classes by selecting “class” in DT table.\r\n\r\n\r\nDT::datatable(tf_idf, filter = 'top') %>% \r\n  formatRound(columns = c('tf', 'idf', \r\n                          'tf_idf'), \r\n              digits = 3) %>%\r\n  formatStyle(0, \r\n              target = 'row', \r\n              lineHeight='25%')\r\n\r\n\r\n\r\n{\"x\":{\"filter\":\"top\",\"filterHTML\":\"<tr>\\n  <td><\\/td>\\n  <td data-type=\\\"character\\\" style=\\\"vertical-align: top;\\\">\\n    <div class=\\\"form-group has-feedback\\\" style=\\\"margin-bottom: auto;\\\">\\n      <input type=\\\"search\\\" placeholder=\\\"All\\\" class=\\\"form-control\\\" style=\\\"width: 100%;\\\"/>\\n      <span class=\\\"glyphicon glyphicon-remove-circle form-control-feedback\\\"><\\/span>\\n    <\\/div>\\n  <\\/td>\\n  <td data-type=\\\"character\\\" style=\\\"vertical-align: top;\\\">\\n    <div class=\\\"form-group has-feedback\\\" style=\\\"margin-bottom: auto;\\\">\\n      <input type=\\\"search\\\" placeholder=\\\"All\\\" class=\\\"form-control\\\" style=\\\"width: 100%;\\\"/>\\n      <span class=\\\"glyphicon glyphicon-remove-circle form-control-feedback\\\"><\\/span>\\n    <\\/div>\\n  <\\/td>\\n  <td data-type=\\\"integer\\\" style=\\\"vertical-align: top;\\\">\\n    <div class=\\\"form-group has-feedback\\\" style=\\\"margin-bottom: auto;\\\">\\n      <input type=\\\"search\\\" placeholder=\\\"All\\\" class=\\\"form-control\\\" style=\\\"width: 100%;\\\"/>\\n      <span class=\\\"glyphicon glyphicon-remove-circle form-control-feedback\\\"><\\/span>\\n    <\\/div>\\n    <div style=\\\"display: none; position: absolute; width: 200px;\\\">\\n      <div data-min=\\\"20\\\" data-max=\\\"1259\\\"><\\/div>\\n      <span style=\\\"float: left;\\\"><\\/span>\\n      <span style=\\\"float: right;\\\"><\\/span>\\n    <\\/div>\\n  <\\/td>\\n  <td data-type=\\\"number\\\" style=\\\"vertical-align: top;\\\">\\n    <div class=\\\"form-group has-feedback\\\" style=\\\"margin-bottom: auto;\\\">\\n      <input type=\\\"search\\\" placeholder=\\\"All\\\" class=\\\"form-control\\\" style=\\\"width: 100%;\\\"/>\\n      <span class=\\\"glyphicon glyphicon-remove-circle form-control-feedback\\\"><\\/span>\\n    <\\/div>\\n    <div style=\\\"display: none; position: absolute; width: 200px;\\\">\\n      <div data-min=\\\"0.015888147442008\\\" data-max=\\\"0.400063552589769\\\" data-scale=\\\"15\\\"><\\/div>\\n      <span style=\\\"float: left;\\\"><\\/span>\\n      <span style=\\\"float: right;\\\"><\\/span>\\n    <\\/div>\\n  <\\/td>\\n  <td data-type=\\\"number\\\" style=\\\"vertical-align: top;\\\">\\n    <div class=\\\"form-group has-feedback\\\" style=\\\"margin-bottom: auto;\\\">\\n      <input type=\\\"search\\\" placeholder=\\\"All\\\" class=\\\"form-control\\\" style=\\\"width: 100%;\\\"/>\\n      <span class=\\\"glyphicon glyphicon-remove-circle form-control-feedback\\\"><\\/span>\\n    <\\/div>\\n    <div style=\\\"display: none; position: absolute; width: 200px;\\\">\\n      <div data-min=\\\"0\\\" data-max=\\\"1.09861228866811\\\" data-scale=\\\"15\\\"><\\/div>\\n      <span style=\\\"float: left;\\\"><\\/span>\\n      <span style=\\\"float: right;\\\"><\\/span>\\n    <\\/div>\\n  <\\/td>\\n  <td data-type=\\\"number\\\" style=\\\"vertical-align: top;\\\">\\n    <div class=\\\"form-group has-feedback\\\" style=\\\"margin-bottom: auto;\\\">\\n      <input type=\\\"search\\\" placeholder=\\\"All\\\" class=\\\"form-control\\\" style=\\\"width: 100%;\\\"/>\\n      <span class=\\\"glyphicon glyphicon-remove-circle form-control-feedback\\\"><\\/span>\\n    <\\/div>\\n    <div style=\\\"display: none; position: absolute; width: 200px;\\\">\\n      <div data-min=\\\"0\\\" data-max=\\\"0.162211811600947\\\" data-scale=\\\"15\\\"><\\/div>\\n      <span style=\\\"float: left;\\\"><\\/span>\\n      <span style=\\\"float: right;\\\"><\\/span>\\n    <\\/div>\\n  <\\/td>\\n<\\/tr>\",\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\"],[\"junk\",\"Typical_Chatter\",\"meaningful\",\"meaningful\",\"Typical_Chatter\",\"junk\",\"junk\",\"meaningful\",\"Typical_Chatter\",\"meaningful\",\"Typical_Chatter\",\"junk\",\"meaningful\",\"Typical_Chatter\",\"junk\",\"meaningful\",\"meaningful\",\"Typical_Chatter\",\"Typical_Chatter\",\"meaningful\",\"Typical_Chatter\",\"Typical_Chatter\",\"Typical_Chatter\",\"Typical_Chatter\",\"Typical_Chatter\",\"Typical_Chatter\",\"meaningful\",\"meaningful\",\"meaningful\",\"Typical_Chatter\",\"Typical_Chatter\",\"junk\",\"junk\",\"junk\",\"meaningful\",\"junk\",\"junk\",\"junk\",\"junk\",\"junk\",\"meaningful\",\"meaningful\",\"junk\",\"meaningful\",\"junk\",\"Typical_Chatter\"],[\"pokrally\",\"tag\",\"abilapost\",\"fire\",\"viktor\",\"kronosstar\",\"life\",\"centralbulletin\",\"newman\",\"apd\",\"park\",\"grammar\",\"dancing\",\"dr\",\"success\",\"dolphin\",\"kronosstar\",\"megaman\",\"swat\",\"van\",\"jakab\",\"sow.kronos\",\"guy\",\"corner\",\"gelato\",\"police\",\"standoff\",\"building\",\"police\",\"people\",\"homelandilluminations\",\"time\",\"rally\",\"person\",\"homelandilluminations\",\"day\",\"failure\",\"successful\",\"world\",\"living\",\"rally\",\"pokrally\",\"people\",\"pok\",\"pok\",\"pok\"],[1259,88,404,378,65,596,202,186,31,146,30,157,140,28,138,127,335,25,25,118,23,22,21,20,20,51,91,89,240,47,44,75,169,58,139,54,53,52,52,50,124,112,70,275,162,57],[0.400063552589768,0.147403685092127,0.139118457300275,0.130165289256198,0.108877721943049,0.189386717508738,0.0641881156657134,0.0640495867768595,0.0519262981574539,0.0502754820936639,0.050251256281407,0.0498887829679059,0.0482093663911846,0.0469011725293132,0.0438512869399428,0.043732782369146,0.115358126721763,0.0418760469011725,0.0418760469011725,0.040633608815427,0.0385259631490787,0.0368509212730318,0.0351758793969849,0.033500837520938,0.033500837520938,0.085427135678392,0.03133608815427,0.0306473829201102,0.0826446280991736,0.0787269681742044,0.0737018425460637,0.0238322211630124,0.0537019383539879,0.0184302510327296,0.0478650137741047,0.0171591992373689,0.0168414362885288,0.0165236733396886,0.0165236733396886,0.0158881474420083,0.0426997245179063,0.0385674931129477,0.0222434064188116,0.0946969696969697,0.0514775977121068,0.0954773869346734],[0.405465108108164,1.09861228866811,1.09861228866811,1.09861228866811,1.09861228866811,0.405465108108164,1.09861228866811,1.09861228866811,1.09861228866811,1.09861228866811,1.09861228866811,1.09861228866811,1.09861228866811,1.09861228866811,1.09861228866811,1.09861228866811,0.405465108108164,1.09861228866811,1.09861228866811,1.09861228866811,1.09861228866811,1.09861228866811,1.09861228866811,1.09861228866811,1.09861228866811,0.405465108108164,1.09861228866811,1.09861228866811,0.405465108108164,0.405465108108164,0.405465108108164,1.09861228866811,0.405465108108164,1.09861228866811,0.405465108108164,1.09861228866811,1.09861228866811,1.09861228866811,1.09861228866811,1.09861228866811,0.405465108108164,0.405465108108164,0.405465108108164,0,0,0],[0.162211811600947,0.161939499837175,0.152837246770632,0.143001186334899,0.119614403288823,0.076789705888931,0.0705178526568027,0.0703656631171723,0.0570468692608231,0.0552332624468127,0.0552066476717643,0.0548084300352378,0.0529634023462587,0.0515262044936467,0.0481755627061326,0.0480453721283918,0.046773695322395,0.0460055397264703,0.0460055397264703,0.0446405819775609,0.0423250965483526,0.0404848749592938,0.038644653370235,0.0368044317811762,0.0368044317811762,0.03463772280321,0.0344262115250682,0.0336695914915502,0.0335095130667904,0.0319210386617818,0.0298835255557106,0.0261823710359416,0.0217742622403177,0.0202477002677948,0.0194075929845161,0.018851307145878,0.0185022088653987,0.0181531105849195,0.0181531105849195,0.0174549140239611,0.0173132484178417,0.0156377727645022,0.00901892518829727,0,0,0]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>class<\\/th>\\n      <th>word<\\/th>\\n      <th>n<\\/th>\\n      <th>tf<\\/th>\\n      <th>idf<\\/th>\\n      <th>tf_idf<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"targets\":4,\"render\":\"function(data, type, row, meta) { return DTWidget.formatRound(data, 3, 3, \\\",\\\", \\\".\\\"); }\"},{\"targets\":5,\"render\":\"function(data, type, row, meta) { return DTWidget.formatRound(data, 3, 3, \\\",\\\", \\\".\\\"); }\"},{\"targets\":6,\"render\":\"function(data, type, row, meta) { return DTWidget.formatRound(data, 3, 3, \\\",\\\", \\\".\\\"); }\"},{\"className\":\"dt-right\",\"targets\":[3,4,5,6]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false,\"orderCellsTop\":true,\"rowCallback\":\"function(row, data) {\\nvar value=data[0]; $(row).css({'line-height':'25%'});\\n}\"}},\"evals\":[\"options.columnDefs.0.render\",\"options.columnDefs.1.render\",\"options.columnDefs.2.render\",\"options.rowCallback\"],\"jsHooks\":[]}\r\nVisualising tf-idf within Three Records Classes\r\nLeveraging facet bar charts to plot the tf-idf values of three class - Junk, Spam, Meaningful records.\r\n\r\n\r\ntf_idf %>%\r\n  group_by(class) %>%\r\n  slice_max(tf_idf, \r\n            n = 12) %>%\r\n  ungroup() %>%\r\n  mutate(word = reorder(word, \r\n                        tf_idf)) %>%\r\n  ggplot(aes(tf_idf, \r\n             word, \r\n             fill = class)) +\r\n  geom_col(show.legend = FALSE) +\r\n  facet_wrap(~ class, \r\n             ncol = 2,\r\n             scales = \"free\") +\r\n  labs(x = \"tf-idf\", \r\n       y = NULL,\r\n       title = \"15 most frequent words in Three Record Classes\") \r\n\r\n\r\n\r\n\r\nQuestion 2\r\nUse visual analytics to represent and evaluate how the level of the risk to the public evolves over the course of the evening. Consider the potential consequences of the situation and the number of people who could be affected. Please limit your answer to 10 images and 1000 words.\r\n\r\n\r\n#fire_count <- data_classed_token %>% \r\n#  filter(str_detect(word,\"fire|Dancing Dolphin Department\")) \r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-07-14-the-sharpe-ratio/the-sharpe-ratio_files/figure-html5/unnamed-chunk-16-1.png",
    "last_modified": "2021-07-21T21:26:02+08:00",
    "input_file": "the-sharpe-ratio.knit.md"
  }
]
