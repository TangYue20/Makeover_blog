[
  {
    "path": "posts/welcome/",
    "title": "Welcome to R for Data Science and Analysis",
    "description": "Welcome to our new blog, R for Data Science and Analysis. We hope you enjoy \nreading what we have to say!",
    "author": [
      {
        "name": "Tang Yue",
        "url": {}
      }
    ],
    "date": "2021-07-14",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-07-14T11:34:38+08:00",
    "input_file": "welcome.knit.md"
  },
  {
    "path": "posts/2021-07-14-the-sharpe-ratio/",
    "title": "VAST Challenge 2021 - Mini-Challenge 3 ",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Tang Yue",
        "url": {}
      }
    ],
    "date": "2021-07-14",
    "categories": [],
    "contents": "\r\n1. Overview\r\nIn the roughly twenty years that Tethys-based GAStech has been operating a natural gas production site in the island country of Kronos, it has produced remarkable profits and developed strong relationships with the government of Kronos. However, GAStech has not been as successful in demonstrating environmental stewardship.\r\nIn January, 2014, the leaders of GAStech are celebrating their new-found fortune as a result of the initial public offering of their very successful company. In the midst of this celebration, several employees of GAStech go missing. An organization known as the Protectors of Kronos (POK) is suspected in the disappearance, but things may not be what they seem.\r\nOn January 23, 2014, multiple events unfolded in Abila. You’ve been asked to come in to perform a retrospective analysis based on limited information about what took place. Your goal is to identify risks and how they could have been mitigated more effectively.\r\nin this assignment we would provide some insights to help law enforcement from Kronos and Tethys.\r\n2. Exploration and preparation of Data\r\nThere are two dataset provides:\r\nMicroblog records that have been identified by automated filters as being potentially relevant to the ongoing incident\r\nText transcripts of emergency dispatches by the Abila, Kronos local police and fire departments.\r\nmaps of Abila and background documents.\r\nTwo dataset are stored in 3 csv files, they are csv-1700-1830.csv“,”csv-1831-2000.csv’ and “csv-2001-2131.csv”:\r\nThe data for Mini-Challenge 3 is found in three segments.\r\nSegment 1 covers the time period from 1700 to 1830 Abila time on January 23.\r\nSegment 2 covers the time period from 1830 to 2000 Abila time on January 23.\r\nSegment 3 covers the time period from 2000 to shortly after 2130 Abila time on January 23.\r\n2.1 Install and load R package\r\nFirst, we run the below code to set the environment.\r\nIn this assignment, the tidyverse, ggforce, GGally, plotly R and parcoords packages will be used, which could be seen from below code chunk.\r\n\r\n\r\npackages = c('tidyverse','dplyr','readr','tm','wordcloud','SnowballC',\r\n             'UpSetR','ggplot2','topicmodels')\r\nfor (p in packages){\r\n  if(!require(p, character.only = T)){\r\n  install.packages(p)\r\n  }\r\n  library(p,character.only = T)\r\n}\r\n\r\n\r\n\r\n2.2 Import dataset and combine data\r\nThe data of Microblog records and emergency calls are stored in separate csv files. And three csv files share same columns but data generate from different date, which is shown below.\r\n1\r\nFirstly, we need to combine 3 files into one consolidated file, which is necessary for the following analysis. In this step, package ‘tidyverse’ would be used.\r\nThe following R code shows the process of data consolidation, then three dataset with different have been integrated into one file.\r\n\r\n\r\nlibrary(tidyverse)\r\ntable1 <- read.csv(\"csv-1700-1830.csv\")\r\ntable2 <- read.csv(\"csv-1831-2000.csv\")\r\ntable3 <- read.csv(\"csv-2001-2131.csv\")\r\ndata <- rbind(table1, table2,table3)\r\n\r\n\r\n\r\nNext, we would separate ‘ccdata’ and ‘mbdata’, they represent microblog record and emergency call center data collected by the Abila, Kronos local police and fire departments.\r\n\r\n\r\nccdata <- subset(data, type == \"ccdata\")\r\nmbdata <- subset(data, type == \"mbdata\")\r\n\r\n\r\n\r\n2.3 Data preprocessing\r\n2.3.1 Modifying Date formate\r\nDate data in table were shown as scientific notation, so we firstly disabling scientific notation in R for further date processing.\r\n\r\n\r\nlibrary(lubridate)\r\n\r\noptions(scipen = 999)\r\nhead(data,5)\r\n\r\n\r\n    type date.yyyyMMddHHmmss.        author\r\n1 mbdata       20140123170000           POK\r\n2 mbdata       20140123170000 maha_Homeland\r\n3 mbdata       20140123170000      Viktor-E\r\n4 mbdata       20140123170000    KronosStar\r\n5 mbdata       20140123170000     AbilaPost\r\n                                                                                                             message\r\n1                                                                                              Follow us @POK-Kronos\r\n2                                       Don't miss a moment!  Follow our live coverage of the POK Rally in the Park!\r\n3                                                        Come join us in the Park! Music tonight at Abila City Park!\r\n4                 POK rally to start in Abila City Park. POK leader Sylvia Marek to open with a speech.\\230 #KronosStar\r\n5 POK rally set to take place in Abila City Park - POK leader Sylvia Marek has begun with opening remarks #AbilaPost\r\n  latitude longitude location\r\n1       NA        NA         \r\n2       NA        NA         \r\n3       NA        NA         \r\n4       NA        NA         \r\n5       NA        NA         \r\n\r\nNext, package ‘lubridate’ would be used to convert data type from ‘yyyymmddhhmmss’ to ‘yyyy-mm-dd hh:mm:ss’, and create a new column ‘date’ in data, and the code chunk could be seen below.\r\n\r\n\r\ndata$date <- ymd_hms(data$date.yyyyMMddHHmmss.)\r\nhead(data$date,5)\r\n\r\n\r\n[1] \"2014-01-23 17:00:00 UTC\" \"2014-01-23 17:00:00 UTC\"\r\n[3] \"2014-01-23 17:00:00 UTC\" \"2014-01-23 17:00:00 UTC\"\r\n[5] \"2014-01-23 17:00:00 UTC\"\r\n\r\n2.3.1 Text proprecessing\r\nSteps of Text preprocessing - Text preprocessing, in general, refers to cleaning of data or making available data available for analysis so further applications like getting the frequency of words\r\nCreating Corpus\r\nUse the following code to load the data as a corpus. A corpus is a collection of text or speech that has been brought together according to be a certain set of predetermined criteria. It arranges text in such a manner so that preprocessing of data can be done or in simple words document term matrix can be created which represent each term present in the text in one hot encoding in the form of a matrix .\r\n\r\n\r\ndata$message <- Corpus(VectorSource(data$message))\r\n\r\ndata$message \r\n\r\n\r\n<<SimpleCorpus>>\r\nMetadata:  corpus specific: 1, document level (indexed): 0\r\nContent:  documents: 4063\r\n\r\nText cleaning\r\nIn this step, we would like to clean text, including punctuation removal, conveting lowe case, stop word removal, extra white space removal and stemming.\r\nCleaning the text data starts with making transformations like removing special characters from the text. This is done using the tm_map() function to replace special characters like / and | with a space. The next step is to remove the unnecessary whitespace and convert the text to lower case.\r\n\r\n\r\n# Replacing \"/\", \"@\" and \"|\" with space\r\ntoSpace <- content_transformer(function (x , pattern ) gsub(pattern, \" \", x))\r\ndata$message  <- tm_map(data$message, toSpace, \"/\")\r\ndata$message  <- tm_map(data$message, toSpace, \"\\\\|\")\r\n# Convert the text to lower case\r\ndata$message  <- tm_map(data$message , content_transformer(tolower))\r\n# Remove english common stopwords\r\ndata$message  <- tm_map(data$message , removeWords, stopwords(\"english\"))\r\n# Remove punctuations\r\ndata$message <- tm_map(data$message , removePunctuation)\r\n# Eliminate extra white spaces\r\ndata$message  <- tm_map(data$message , stripWhitespace)\r\n# Text stemming - which reduces words to their root form\r\ndata$message  <- tm_map(data$message , stemDocument)\r\n\r\n\r\n\r\nNext, we would use topic modeling to explore topics involved in microblog and call center records. Latent Dirichlet allocation (LDA) is a particularly popular method for fitting a topic model, here topicmodels package and present topic result with ggplot2 and dylyr.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-07-14T14:29:42+08:00",
    "input_file": "the-sharpe-ratio.knit.md"
  }
]
